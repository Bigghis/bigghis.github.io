---
title: "Evaluation"
description: Evaluation of a fine tuned model by humans 
date: 2025-06-06 12:00:00 +0530
categories: [Fine Tuning, Smart form, Evaluation]
tags: [a.i., fine tuning, evaluation]
comments: false
---

### Simple human evaluation of a fine tuned model


Now that we have a [fine tuned model](https://bigghis.github.io/posts/FINETUNING/) we need to evaluate it, to test if it's better than the original model.


### Evaluation process

We want compare the performance of the fine tuned model (**Llama-3.2-3B-Instruct LoRA**) with the original model (**Llama-3.2-3B-Instruct**) in the task of form filling.

A simple way to evaluate the model is to ask a human to check visually if the model is better than the original model.  
So, to do this in a systematic way, we can use **a list of models outputs for the same user prompts** to evaluate the model.  

Starting from our synthetic dataset, created by ChatGPT 3.5 Turbo, used to train the LoRA model, we can create a csv file containing the following columns:

**1) from synthetic dataset:**
- `system_prompt`: Contains the instructions that are given to the model. It defines the fields to be extracted.
- `user_prompt`: Contains the user's text from which the information should be extracted.
- `expected_completion`: This is the "ground truth" - the ideal output we expect from the model as created by ChatGPT 3.5 Turbo.

**2) from base model inference:**
- `base_result`: The output generated by the original base model (in this specific case: Llama-3.2-3B-Instruct).

**3) from fine tuned (LoRA) model inference:**
- `lora_result`: The output from our new, fine-tuned model (in this specific case: Llama-3.2-3B-Instruct LoRA).

This allows for a side-by-side comparison between the base model and the fine-tuned model against the expected outcome.

### Creating the csv file

We have a lot of objects with different texts but same structure, in our synthetic dataset:

```python
prompts = [{
  "system_prompt": {
    "role": "system",
    "content": "\nEach response line matches the following format:\nFIELD identifier^^^value\n\nGive a response with the following lines only, with values inferred from USER_DATA:\n\nFIELD address^^^The address of type string\nFIELD property_type^^^The property_type of type string\nFIELD size^^^The size of type integer\nFIELD kitchen^^^The kitchen of type boolean\nFIELD bathroom_window^^^The bathroom_window of type boolean\nFIELD floor^^^The floor of type integer\nFIELD elevator^^^The elevator of type boolean\nEND_RESPONSE\n\nDo not explain how the values were determined.\nFor fields without any corresponding information in USER_DATA, use value NO_DATA.\n"
  },
  "user_prompt": {
    "role": "user",
    "content": "\nUSER_DATA:La casa in affitto si trova in Via Roma, 12. E' un monolocale di 40 metri quadrati con cucina a vista e bagno finestrato. E' situato al secondo piano di un edificio con ascensore.\n"
  },
  "expected_completion": {
    "role": "assistant",
    "content": "FIELD address^^^Via Roma, 12\nFIELD property_type^^^monolocale\nFIELD size^^^40\nFIELD kitchen^^^True\nFIELD bathroom_window^^^True\nFIELD floor^^^2\nFIELD elevator^^^True\n"
  }
}, {
  ...
}, ...]
```
We need to create a script to run inference from the prompts for base and fine tuned models and then save the results in a csv file.


```python
# 3B model
BASE_MODEL = "meta-llama/Llama-3.2-3B-Instruct"
LORA_DIR = "/outputs/lora-out/checkpoint-764" # fine tuned model at certain checkpoint

# models inference

#firstly we use base model inference
base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)

base_results = run_model(prompts, base_model, tokenizer, "base_model", limit)

# then we use fine tuned model:
# Load the LoRA weights and merge them with the base model:
lora_model = PeftModel.from_pretrained(base_model, LORA_DIR)
lora_model = lora_model.merge_and_unload()

lora_results = run_model(prompts, lora_model, tokenizer, "lora_model", limit)

# now we can create the csv file:
with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
    csvwriter = csv.writer(csvfile)
    csvwriter.writerow(['system_prompt', 'user_prompt', 'expected_completion', 'base_result', 'lora_result'])
    for prompt, base_result, lora_result in zip(prompts, base_results, lora_results):
        csvwriter.writerow([
            prompt['system_prompt']['content'], 
            prompt['user_prompt']['content'], 
            prompt['expected_completion']['content'], 
            base_result, 
            lora_result
        ])

```

It produces a csv file like this, so a human can view the results and the differences between the models inference responses to decide if the fine tuned model is acceptably better than the base model or not.

![Human Eval](/assets/images/exampleHumanEvaluation.png)

This is a simple way to evaluate the model, but generally speaking, evaluation is a complicated topic and there are many aspects to consider.
There is a lot of literature on **benchmarks** that is worth looking into, 
but for a first simple evaluation of small models on specific tasks, a comparison like the one presented can be useful.






