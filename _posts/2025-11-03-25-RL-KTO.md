---
title: "Reinforcement Learning with KTO"
description: Reinforcement Learning con KTO 
date: 2025-11-03 12:00:00 +0530
categories: [Alignment, Reinforcement Learning]
tags: [HALO, KTO, Alignment, reinforcement learning]
comments: false
---

## Alignment con HALO

Nell'ambito del **reinforcement learning**, gli algoritmi di tipo **HALO** (Human-Aware Loss Function) svolgono un ruolo importante per l'alignment dei modelli.
La caratteristica di tali algoritmi fa sì che **non bisogna definire a priori una funzione di reward**, ma si può utilizzare **un dataset di preferenze** per istruire il modello.    
In sostanza un algoritmo HALO apprende una funzione di reward basandosi su un dataset di preferenze esaminato in offline durante il training.    
L'approccio è nato nell'ambito della [navigazione robotica](https://arxiv.org/html/2508.01539v1){:target="_blank"} ma è stato subito esteso anche per l'alignment di modelli LLM.  

Tra i vari algoritmi di tipo HALO che sono stati creati per i modelli LLM, quello che esamino in questo post è il **KTO** (**K**ahneman‑**T**versky **O**ptimization). 


## KTO

L'algoritmo è descritto nel paper [KTO: Model Alignment as Prospect Theoretic Optimization](https://arxiv.org/html/2402.01306v1){:target="_blank"}.   
Prende spunto dalla **teoria dei prospetti** di Kahneman e Tversky, in particolare dal concetto di **avversione alle perdite**.  
In ambito finanziario, per esempio, le persone hanno la tendenza a provare un dolore psicologico maggiore per una perdita, rispetto al piacere di un guadagno di pari ammontare.
Questo **bias** influenza la nostra percezione e porta a proteggerci dalle perdite evitando rischi che, tuttavia, implicano la rinuncia a possibilità di guadagno maggiori. 

Per quanto riguarda l'implementazione dell'algoritmo, KTO prevede di usare un **dataset di preferenze binario** (buono/cattivo) che semplifica la raccolta dei feedback, al contrario degli altri algoritmi HALO che usano **coppie di preferenze**.  

Esempio, un classico algoritmo HALO che è il **DPO (Direct Preference Optimization)** richiede di avere una coppia di preferenze (chosen/rejected) per lo stesso prompt.  

```json[
  {
    "prompt": "Qual è la capitale d'Italia?",
    "chosen": "Roma.",
    "rejected": "Milano."
  },
  {
    "prompt": "Riassumi in una frase: 'Il sonno migliora memoria e salute'.",
    "chosen": "Il sonno aiuta memoria e salute, quindi è importante dormire bene.",
    "rejected": "Dormire è noioso e fa perdere tempo."
  }
]
```
invece un prompt KTO è più semplice ed ha una sola preferenza etichettata come vero/falso:
```json[
  {
    "prompt": "Qual è la capitale d'Italia?",
    "completion": "Roma.",
    "label": true
  },
  {
    "prompt": "Riassumi in una frase: 'Il sonno migliora memoria e salute'.",
    "completion": "Dormire è noioso e fa perdere tempo.",
    "label": false
  }
]
```

In generale avremo un dataset composto da un certo numero di preferenze positive e negative.  
L'algoritmo KTO premia le preferenze positive con un peso crescente (**peso di confidenza**) quando il modello è incerto nella sua risposta e punisce le preferenze negative con maggiore forza (**avversione alle perdite**) quando il modello le considera probabili.  


## Iperparametri

Dal punto di vista operativo nel paper KTO sono descritti gli iperparametri che possono essere utilizzati per configurare l'algoritmo, rapportandone l'entità con le prove sperimentali fatte su addestramenti di **Llama 3.1 8B** e **Qwen2.5 3B Instruct**.  

I valori di partenza suggeriti sono:  
**Learning Rate**: 5e-6 se si usa un optimizer di tipo **AdamW**.  
**Epochs**: 2, 3 sono sufficienti.  
**Batch Size**: per un corretto funzionamento usare batch compresi tra 8 e 128. 
32 potrebbe essere un buon punto di partenza.  
**Beta**: (regolatore di stabilità), usato per il concetto di **avversione al rischio**, decide quanto il modello che si sta allineando deve essere punito se si allontana dal modello di riferimento addestrato tramite SFT.  
β basso (vicino a 0) significa che il modello è libero di allontanarsi dal riferimento, per massimizzare le ricompense.
β alto (vicino a 1) significa che il modello è penalizzato se si allontana dal riferimento, per minimizzare le perdite e garantire la stabilità, limitando potenziali miglioramenti del modello stesso.  
Nel paper KTO si suggerisce un valore di 0.1.  
**desiderable/undesirable weights**: pesi usati per regolare l'**avversione alle perdite**, dando maggiore o minore importanza alle preferenze positive e negative.  

Il rapporto tra i pesi è **cruciale** soprattutto in presenza di **dataset sbilanciati** nel numero di preferenze positive e negative.  
possiamo regolare i pesi rispettando il range suggerito dalla formula mostrata nel paper:

![KTO Loss Aversion](/assets/images/kto_avversion_loss.png)  
con:  
nD = numero preferenze positive, nU = numero preferenze negative
λD = peso preferenze positive, λU = peso preferenze negative 

per cui, se abbiamo un dataset bilanciato possiamo usare 
λU = 1, λD = [10, 15]
se abbiamo un dataset sbilanciato con rapporto 1:10 (un esempio positivo ogni 10 negativi) 
λD = 1, λU = 10.  
oppure se abbiamo: 
nD = 9000, nU = 3000
λD = 1, λU = [2, 3].


Si è sperimentato che KTO riesce a gestire forti squilibri tra numeri di preferenze positive e negative, anche con molte meno preferenze positive.


## Implementazione con TRL e axolotl

Per usare KTO con TRL usa la classe [KTOTrainer](https://huggingface.co/docs/trl/v0.24.0/en/kto_trainer#trl.KTOTrainer){:target="_blank"} e KTOConfig.  

Axolotl [fornisce supporto per KTO](https://docs.axolotl.ai/docs/rlhf.html#kto){:target="_blank"} (wrapper per TRL) attraverso diversi formati di dataset.  
Per un formato custom del genere:

```json
{"system":"You are Pasquale, chatting with Lorenzo. Respond naturally in their conversational style.","prompt":"Daje giù","completion":"Non mi interessa.","label":false}
{"system":"You are Pasquale, chatting with Lorenzo. Respond naturally in their conversational style.","prompt":"Daje giù","completion":"eh :(","label":true}
```
per il RL con KTO, possiamo usare la configurazione:

```yaml
base_model: meta-llama/Llama-3.1-8B-Instruct
rl: kto
rl_beta: 0.1 #default
kto_desirable_weight: 1.0    # puoi aumentare se hai pochi "desiderabili"
kto_undesirable_weight: 1.0  # puoi aumentare se hai pochi "indesiderabili"
remove_unused_columns: false

datasets:
  - ds_type: json
    data_files:
      - /abs/path/dataset-kto-labeled.jsonl
    split: train
    type:
      field_prompt: "prompt"
      field_system: "system"
      field_completion: "completion"
      field_label: "label"
      prompt_format: "{prompt}"
      completion_format: "{completion}"

gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: true

```

### Dataset per KTO
Generare un dataset per KTO dipende dalla situazione specifica per cui si sta allineando il modello.  
Dopo aver eseguito un SFT, possiamo generare un dataset per il RL con KTO, a partire dal dataset di SFT.  
Potremmo costruire il dataset KTO generando preferenze binarie positive a partire dal dataset di SFT e preferenze negative sintetiche per inferenza del modello base ante SFT, magari con sampling aggressivi (parametri di temperatura alta, top-p basso etc.) per esempio. 

Tornando all'esempio del dataset costruito dalle chat telegram del progetto [LLM "Chat Like Me"](https://bigghis.github.io/posts/2025-16-10-25-LLM-CHAT-LIKE-ME/), possiamo fare diversi ragionamenti per generare dataset di preferenze negative:

**Base model con sampling aggressivo**: genera risposte dal modello base (pre‑SFT) con parametri che aumentano la variabilità delle risposte (parametri di temperatura alta, top-p basso etc.)

es.: tono generico/formale:
```json
{
  "prompt": "Daje giù",
  "completion": "La ringrazio per il suo messaggio. Valuterò attentamente.",
  "label": false
}

```

**Shuffle/Corruzione di risposte reali**: altera le tue risposte (ordine parole, rimozione emoji/tono, formalizzazione forzata).

es.: contenuto simile ma stile diverso dal voluto:
```json
{
  "prompt": "eh :(",
  "completion": "Certamente, capisco perfettamente la sua situazione.",
  "label": false
}
```

**Contaminazione con altri dataset o/e template generici/formali**: usa risposte di altri interlocutori/persone su prompt simili, oppure frasi standard, neutrali o eccessivamente formali non in linea con il tuo stile.
```json
{
  "prompt": "Come va?",
  "completion": "La ringrazio per il suo messaggio. Valuterò attentamente.",
  "label": false
}
```

**Paraphrasing negativo**: parafrasa una tua risposta rendendola più lunga, neutra o troppo professionale.
```json
{
  "prompt": "ok dai",
  "completion": "ahahah! Comprendo il tuo punto di vista e procederò di conseguenza.",
  "label": false
}
```

- **Template contestuali (augmentation)**: riempi schemi generici con parti del prompt, mantenendo il contenuto ma perdendo il tuo stile.
```json
{
  "prompt": "Su {topic} capisco la tua posizione. {conclusione neutra}.",
  "completion": "La ringrazio per il suo messaggio. Valuterò attentamente.",
  "label": false
}
```

Questi sono solo alcuni esempi, importante è adattare il dataset alle esigenze volute.


